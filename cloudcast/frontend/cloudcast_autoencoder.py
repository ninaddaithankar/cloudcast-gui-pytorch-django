# -*- coding: utf-8 -*-
"""cloudcast-autoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17s8_w0Sq7-JnXV5lU-QXikrJNB3wpwGL

# Scripts
( for importing raw dataset and installing libraries. )
"""
from pyresample.geometry import AreaDefinition
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import argparse
from collections import OrderedDict
from pytorch_lightning import Trainer
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torch.autograd import Variable
import torch.nn as nn
import torch
import xarray as xr
import sys
import os


def pre_process_data(vals, nan_to_zero, normalize, restrict_classes_to_4, fill_value=np.nan):
    if np.isnan(fill_value):
        vals = vals.astype('float')
    vals[vals == 0] = fill_value  # nothing
    vals[vals == 1] = fill_value  # cloud-free land
    vals[vals == 2] = fill_value  # Cloud-free sea
    vals[vals == 3] = fill_value  # snow over land
    vals[vals == 4] = fill_value  # sea ice

    vals[vals > 230] = fill_value  # these should not be included

    if nan_to_zero:
        if np.isnan(fill_value):
            vals[np.isnan(vals)] = 0
        else:
            vals[vals < 0] = 0

        vals[vals > 0] = vals[vals > 0] - 4

    if restrict_classes_to_4:
        # fractional clouds are set to low-level cloud  --> CTTH is not available for clouds classified as fractional.
        vals[vals == 6] = 1
        # however, fractional clouds = low level clouds, will in most cases match reality
        #  as fractional clouds are typically small cumulus and stratocumulus clouds
        # that is, clouds between 300m and 3km height

        # we define low-, mid- and high-level clouds
        vals[(vals > 0) & (vals < 3)] = 1  # low-level clouds
        vals[vals == 3] = 2               # mid-level clouds
        vals[(vals > 3)] = 3              # high-level clouds

    if normalize == True:
        if restrict_classes_to_4:
            vals = 2 * (vals / 3) - 1  # normalize to be between [-1, 1}
        else:
            vals = 2 * (vals / 10) - 1  # normalize to be between [-1, 1}
    return vals


def str2bool(v):
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def make_one_hot(labels, C=4):

    one_hot = torch.FloatTensor(labels.size(0), C, labels.size(
        1),  labels.size(2), labels.size(3)).zero_()
    return one_hot.scatter_(1, labels.data.unsqueeze(1), 1)


def make_one_hot_reduced(labels, C=4):

    one_hot = torch.FloatTensor(
        C, labels.size(0),  labels.size(1)).zero_()
    return one_hot.scatter_(0, labels.long().data.unsqueeze(0), 1)


# Change colors for matplotlib CM


def save_images_cartopy(prediction_video, lines=False, high_res_map=True, name='predicted', path="/data/output/"):
    blues = mpl.cm.get_cmap('Blues', 12)
    newcolors = blues(np.linspace(0, 1, 256))
    newcolors[:, :] *= 0.9  # make colors less white
    newcmp = mpl.colors.ListedColormap(newcolors)

    height = 128   # 18000 resolution
    width = 128

    # projection parameters
    lower_left_xy = [-855100.436345, -4942000.0]
    upper_right_xy = [1448899.563655, -2638000.0]
    area_def = AreaDefinition('areaD', 'Europe', 'areaD',
                              {'lat_0': '90.00', 'lat_ts': '50.00',
                               'lon_0': '5', 'proj': 'stere', 'ellps': 'WGS84'},
                              height, width,
                              (lower_left_xy[0], lower_left_xy[1],
                               upper_right_xy[0], upper_right_xy[1]))

    crs = area_def.to_cartopy_crs()

    create_video(prediction_video, name,
                 crs, newcmp, lines, high_res_map, path)


def create_video(video, name, crs, newcmp, lines=False, high_res_map=False, path="/data/output/"):
    # Create folder for batch images if not exist
    if not os.path.exists(path):
        os.makedirs(path)

    for frame in np.arange(0, video.shape[1]):
        img = video[0, frame, :, :].detach().cpu().numpy()
        # renormalize for tanh
        #img = np.round((img + 1) * 14 / 2,0)
        img = img.astype('float')
        img[img == 0] = np.nan  # no cloud we want to be transparent

        plt.figure(figsize=(8, 6))
        ax = plt.axes(projection=crs)

        if lines:
            ax.coastlines()
            ax.gridlines()
            ax.set_global()
        if high_res_map:
            # BM is a custom image collected from https://neo.sci.gsfc.nasa.gov/view.php?datasetId=BlueMarbleNG-TB
            ax.background_img(name='BM', resolution='low')

        plt.imshow(img, cmap=newcmp, transform=crs,
                   extent=crs.bounds, origin='upper')
        plt.savefig(path + '/' + name + '-' + str(frame) + '.png')

        plt.close()


"""# Pytorch dataset container"""


class CloudDataset(Dataset):
    """Cloud-labelled satellite images."""

    def __init__(self, nc_filename, root_dir, transform=None, frames=12, n_lags=1, restrict_classes=False,
                 normalize=True, nan_to_zero=True, return_timestamp=False, sequence_start_mode='unique', clip_dataset=True):

        # Load dataset into memory (do not do this for full dataset!
        self.cloud_frames = xr.open_dataarray(nc_filename).load()
        # np.load(nc_filename)

        # config parameters
        self.root_dir = root_dir
        self.transform = transform
        self.frames = frames
        self.n_lags = n_lags
        self.restrict_classes = restrict_classes
        self.normalize = normalize
        self.nan_to_zero = nan_to_zero
        self.return_timestamp = return_timestamp
        self.sequence_start_mode = sequence_start_mode

        assert sequence_start_mode in [
            'all', 'optimal', 'unique'], 'sequence mode must be one of "all", "optimal" or "unique"'
        assert n_lags <= frames // 2, 'Mismatch between number of frames and lags, lags should not exceed 50% of n_frames'

        if self.sequence_start_mode == 'all':  # include all sequences despite potential overlaps
            self.possible_starts = np.array(
                [i for i in range(len(self.cloud_frames.time) - self.frames)])
            if clip_dataset == True:
                self.possible_starts = self.possible_starts[:1000]
        elif self.sequence_start_mode == 'unique':  # do no allow overlaps
            # unique as ???
            self.possible_starts = np.array([i for i in range(len(self.cloud_frames.time) - self.frames)
                                             if i % self.n_lags == 0])
            if clip_dataset == True:
                self.possible_starts = self.possible_starts[:1000]
            print(len(self.possible_starts))
        else:
            # Optimal as we are not using all data (which gives too much emphasis ???? Shouldn't we just use all=
            pass
        # clipping dataset possible starts for easier training and dataloading

    def __len__(self):
        return len(self.possible_starts)
        # return len(self.cloud_frames.time)

    def __getitem__(self, idx):
        index = self.possible_starts[idx]

        # clip  # clip corresponds to an array of 32 frames
        clip = self.cloud_frames[:, :, index:index + self.frames]

        if self.transform is not None:
            clip.values = self.transform(clip.values, nan_to_zero=self.nan_to_zero, normalize=self.normalize,
                                         restrict_classes_to_4=self.restrict_classes)

        # Save time stamps
        times = clip.time.values

        clip = np.array(clip)
        # transform into torch.tensors
        clip = torch.stack([torch.Tensor(i) for i in clip])

        # Specify X and y according to number of lags
        y = clip[:, :, self.n_lags:]
        X = clip[:, :, 0:self.n_lags]

        # rearrange to have time steps, height and width as the shape
        y = y.permute(2, 0, 1)
        # rearrange to have time steps, height and width as the shape
        X = X.permute(2, 0, 1)

        if self.return_timestamp:
            ts = times.astype('datetime64[s]').astype('int')
            ts = torch.tensor(ts, dtype=torch.int64)

            return X, y, ts
        else:
            return X, y


"""# Model Definition

### 1. Individual ConvLSTM Cell
"""


class ConvLSTMCell(nn.Module):

    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias):

        super(ConvLSTMCell, self).__init__()

        self.height, self.width = input_size
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        self.kernel_size = kernel_size
        self.padding = kernel_size[0] // 2, kernel_size[1] // 2
        self.bias = bias

        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,
                              out_channels=4 * self.hidden_dim,
                              kernel_size=self.kernel_size,
                              padding=self.padding,
                              bias=self.bias)  # .cuda()

    def forward(self, input_tensor, cur_state):
        h_cur, c_cur = cur_state

        # concatenate along channel axis
        combined = torch.cat([input_tensor, h_cur], dim=1)

        combined_conv = self.conv(combined)
        cc_i, cc_f, cc_o, cc_g = torch.split(
            combined_conv, self.hidden_dim, dim=1)
        i = torch.sigmoid(cc_i)
        f = torch.sigmoid(cc_f)
        o = torch.sigmoid(cc_o)
        g = torch.tanh(cc_g)

        c_next = f * c_cur + i * g
        h_next = o * torch.tanh(c_next)

        return h_next, c_next

    def init_hidden(self, batch_size):
        zeros = Variable(torch.zeros(
            batch_size, self.hidden_dim, self.height, self.width))
        return (Variable(torch.nn.init.xavier_uniform_(zeros)),  # .cuda(),  # .half(),
                Variable(torch.nn.init.xavier_uniform_(zeros)))  # .cuda())  # .half())
        # TRY TO SEE WHAT THIS RETURNS


class up(nn.Module):
    def __init__(self, in_ch, out_ch, bilinear=True):
        super(up, self).__init__()

        if bilinear:
            self.up = nn.Upsample(
                scale_factor=2, mode='bilinear', align_corners=True)
        else:
            self.up = nn.ConvTranspose2d(
                in_ch//2, in_ch//2, 2, stride=2)  # .cuda()

        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)) \
            # .cuda()

    def forward(self, x1, x2):

        x1 = self.up(x1)

        x = torch.cat([x2, x1], dim=1)
        x = self.conv(x)
        return x


"""### 2. DeepAutoencoder using those ConvLSTM cells"""


class DeepAutoencoderConvLSTM(nn.Module):
    def __init__(self, nf=64, in_chan=1, input_size=(16, 16), n_classes=4):

        super(DeepAutoencoderConvLSTM, self).__init__()

        self.convlstm_initial = ConvLSTMCell(input_size=input_size,
                                             input_dim=nf * 4,
                                             hidden_dim=nf * 4,
                                             kernel_size=(3, 3),
                                             bias=True)

        self.convlstm_intermediary = ConvLSTMCell(input_size=input_size,
                                                  input_dim=nf * 4,
                                                  hidden_dim=nf * 4,
                                                  kernel_size=(3, 3),
                                                  bias=True)

        self.convlstm_final = ConvLSTMCell(input_size=input_size,
                                           input_dim=nf * 4,
                                           hidden_dim=nf * 4,
                                           kernel_size=(3, 3),
                                           bias=True)

        self.encoder1 = nn.Sequential(
            nn.Conv2d(in_channels=in_chan,     # could set this five
                      out_channels=nf,  # 11
                      bias=False,
                      kernel_size=(3, 3),
                      padding=(1, 1),
                      stride=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(nf)
        )
        self.encoder2 = nn.Sequential(
            nn.Conv2d(in_channels=nf,  # could set this to 4 and then use one-hot encoded inputs!
                      out_channels=nf*2,  # 11
                      bias=False,
                      kernel_size=(3, 3),
                      padding=(1, 1),
                      stride=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(nf*2)
        )

        self.encoder3 = nn.Sequential(
            nn.Conv2d(in_channels=nf*2,  # could set this to 4 and then use one-hot encoded inputs!
                      out_channels=nf*4,  # 11
                      bias=False,
                      kernel_size=(3, 3),
                      padding=(1, 1),
                      stride=(1, 1)),
            nn.ReLU(),
            nn.BatchNorm2d(nf*4)
        )
        self.downsampler = nn.MaxPool2d(2)
        #           # try convolution with stride 2

        self.decoder1 = up(512, 128, bilinear=False)
        self.decoder2 = up(256, 64, bilinear=False)
        self.decoder3 = up(128, 4, bilinear=False)

        self.layernorm1 = torch.nn.LayerNorm([64, 128, 128])  # .cuda()
        self.layernorm2 = torch.nn.LayerNorm([128, 64, 64])  # .cuda()
        self.layernorm3 = torch.nn.LayerNorm([256, 32, 32])  # .cuda()

    def autoencoder(self, x, seq_len, h_t, c_t, h_t2, c_t2, h_t3, c_t3, future=False, metachannels=None):
        outputs = []

        if future:
            output = x

        for t in range(seq_len):
            if future:
                output = torch.softmax(output, dim=1)
                output = torch.argmax(output, dim=1) \
                    .unsqueeze(1)
                output = make_one_hot(output.long(), 4) \
                    .squeeze(2)  # THIS DOES NOT WORK WITH METACHANNEL!

                if metachannels is not None:
                    output = torch.cat(
                        [output, metachannels[:, t, :, :].unsqueeze(1)], 1)  # .cuda()

                x1_c = self.encoder1(output)
                # x1_c = self.layernorm1(x1_c)

            else:
                # 4, 16, 5, 128, 128  == correct?
                x1_c = self.encoder1(x[:, t, :, :, :])
                # x1_c = self.layernorm1(x1_c)

            x1 = self.downsampler(x1_c)

            x2_c = self.encoder2(x1)
            # x2_c = self.layernorm2(x2_c)
            x2 = self.downsampler(x2_c)

            x3_c = self.encoder3(x2)
            # x3_c = self.layernorm3(x3_c)
            x3 = self.downsampler(x3_c)

            h_t, c_t = self.convlstm_initial(input_tensor=x3,
                                             cur_state=[h_t, c_t])  # we could concat to provide skip conn here
            h_t2, c_t2 = self.convlstm_intermediary(input_tensor=h_t,
                                                    cur_state=[h_t2, c_t2])
            h_t3, c_t3 = self.convlstm_final(input_tensor=h_t2,
                                             cur_state=[h_t3, c_t3])
            # print(h_t3.shape)
            output = self.decoder1(h_t3, x3_c)
            # print(output.shape)
            output = self.decoder2(output, x2_c)
            # print(output.shape)
            # maybe 4 to 64 is a bit rough
            output = self.decoder3(output, x1_c)
            # print(output.shape)

            outputs += [output]

        return outputs, output

    def forward(self, x, future_seq=0, metachannels=None):
        """
        Parameters
        ----------
        input_tensor:
            5-D Tensor of shape (t, b, c, h, w)
        """
        # initialize hidden state
        # TODO: Try similar convolutions between encoder and decoder
        # maybe do not initialize to zero but do glorot
        h_t, c_t = self.convlstm_initial.init_hidden(x.size(0))
        h_t2, c_t2 = self.convlstm_intermediary.init_hidden(x.size(0))
        h_t3, c_t3 = self.convlstm_final.init_hidden(x.size(0))

        seq_len = x.size(1)

        # METACHANNEL
        if x.shape[2] > 1:
            x_onehot = make_one_hot(x[:, :, 0, :, :].squeeze(2).long(), 4) \
                .permute(0, 2, 1, 3, 4)
            x = torch.cat([x_onehot, x[:, :, 1:, :, :]],
                          2)  # MAKE SURE THIS WORKS
        else:
            x = make_one_hot(x.squeeze(2).long(), 4)  # DO THIS FOR MSE!
            x = x.permute(0, 2, 1, 3, 4)

        # input sequence length
        # torch.Size([4, 16, 5, 128, 128])
        outputs, output = self.autoencoder(
            x, seq_len, h_t, c_t, h_t2, c_t2, h_t3, c_t)
        outputs = torch.stack(outputs, 1)

        # Future predictions based on transformed input
        if future_seq != 0:
            outputs_future, _ = self.autoencoder(output, future_seq, h_t, c_t, h_t2, c_t2, h_t3, c_t, future=True,
                                                 metachannels=metachannels)
            outputs_future = torch.stack(outputs_future, 1)
            outputs = torch.cat([outputs, outputs_future], 1)

        # .squeeze(2)            # torch.Size([2, 128, 16, 128, 128])
        outputs = outputs.permute(0, 2, 1, 3, 4)

        return outputs


"""### 3. Final ConvLSTM Model"""


class ConvLSTMModel(pl.LightningModule):
    def __init__(self, hparams=None, path="./cloudcast-dataset/"):
        super(ConvLSTMModel, self).__init__()
        self.hparams.update(vars(hparams))

        self.batch_size = hparams.batch_size
        self.num_classes = hparams.num_classes
        self.num_channels_in = hparams.num_channels_in
        self.future_steps = hparams.future_steps
        self.num_workers = hparams.num_workers

        self.path = path

        self.model = DeepAutoencoderConvLSTM(in_chan=self.num_channels_in,
                                             n_classes=self.num_classes)

        if hparams.loss == 'l1':
            raise NotImplementedError('L1 loss not implemented yet')
            # self.criterion = torch.nn.L1Loss()
        elif hparams.loss == 'ce':
            self.criterion = torch.nn.CrossEntropyLoss()
        elif hparams.loss == 'weighted_ce':
            self.criterion = torch.nn.CrossEntropyLoss(
                weight=torch.tensor([1 / 0.3, 1 / 0.3, 1 / 0.1, 1 / 0.3]),
                reduction='mean')
        elif hparams.loss == 'combined':
            raise NotImplementedError('combined loss not implemented yet')
            # self.a_loss = torch.nn.L1Loss()#.cuda()
            # self.b_loss = torch.nn.CrossEntropyLoss(
            #     weight=torch.tensor([1 / 0.3, 1 / 0.3, 1 / 0.1, 1 / 0.3]),
            #     reduction='mean')

    def forward(self, x):
        x = x.unsqueeze(2)
        #x = x.to(device='cuda')

        output = self.model(x, future_seq=self.future_steps - 1)
        probas = F.softmax(output, dim=1)
        return probas

    def inference(self, x):
        x = x.unsqueeze(2)

        output = self.model(x, future_seq=self.future_steps - 1)
        probas = F.softmax(output, dim=1)
        return probas

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.forward(x)
        y_hat = y_hat[:, :, -self.future_steps:, :, :]

        if hparams.loss == 'combined':
            a_loss = self.a_loss(y_hat, y)
            b_loss = self.b_loss(y_hat, y)
            loss = a_loss + b_loss
        else:
            loss = self.criterion(y_hat, y.long())
            # loss = F.cross_entropy(y_hat, y.long())

        y_hat_class = torch.argmax(y_hat, 1)
        hits = y_hat_class == y
        accuracy = np.mean(hits.detach().cpu().numpy())
        accuracy = torch.tensor(accuracy)  # .cuda()
        self.log('train_loss', loss, on_step=True,
                 on_epoch=True, prog_bar=True, logger=True)
        self.log('accuracy', accuracy, on_step=True,
                 on_epoch=True, prog_bar=True, logger=True)

        return {'loss': loss}

    def test_step(self, batch, batch_idx):
        x, y, timestamp = batch
        y_hat = self.forward(x)
        y_hat = y_hat[:, :, -self.future_steps:, :, :]
        loss = F.cross_entropy(y_hat, y.long())

        y_hat_class = torch.argmax(y_hat, 1)
        hits = y_hat_class == y
        accuracy = np.mean(hits.detach().cpu().numpy())
        accuracy = torch.tensor(accuracy)  # .cuda()

        output = OrderedDict({
            'loss': loss,
            'accuracy': accuracy
        })

        return output

    def test_end(self, outputs):
        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
        avg_accuracy = torch.stack([x['accuracy'] for x in outputs]).mean()
        tensorboard_logs = {'test_loss': avg_loss, 'accuracy': avg_accuracy}
        return {'avg_test_loss': avg_loss, 'avg_accuracy': avg_accuracy, 'log': tensorboard_logs}

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, betas=(self.hparams.beta1, hparams.beta2),
                                     eps=self.hparams.eps)
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100)
        return [optimizer], [scheduler]

    def train_dataloader(self):
        # Initialize data loader
        train_set = CloudDataset(nc_filename=self.path + 'TrainCloud.nc',
                                 root_dir=self.path,
                                 sequence_start_mode='unique',
                                 n_lags=4,
                                 transform=pre_process_data,
                                 normalize=False,
                                 nan_to_zero=True,
                                 restrict_classes=True,
                                 return_timestamp=False,
                                 frames=8)

        train_loader = torch.utils.data.DataLoader(
            dataset=train_set,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=True)

        return train_loader

    def test_dataloader(self):
        test_set = CloudDataset(nc_filename=self.path + 'TrainCloud.nc',
                                root_dir=self.path,
                                sequence_start_mode='unique',
                                n_lags=4,
                                transform=pre_process_data,
                                normalize=False,
                                nan_to_zero=True,
                                return_timestamp=True,
                                restrict_classes=True,
                                frames=8)

        test_loader = torch.utils.data.DataLoader(
            dataset=test_set,
            batch_size=self.batch_size,
            num_workers=0,
            shuffle=False
        )
        return test_loader


def train_model(model, config):
    trainer = Trainer(log_every_n_steps=5, resume_from_checkpoint=config.pretrained_path,
                      max_epochs=config.max_epochs, gpus=config.num_gpus, distributed_backend=config.data_backend)
    trainer.fit(model)
    trainer.save_checkpoint(os.getcwd() + '/checkpoints/ae-convlstm.ckpt')
    trainer.save_checkpoint(config.pretrained_path)


def save_predictions(trainer, model, imgs_X, save_images_path):
    print('Creating predictions...')
    model.cpu()

    imgs_X_tensor = torch.tensor(imgs_X)
    pred = model.forward(imgs_X_tensor)
    pred = pred[:, :, -4:, :, :]
    # .cuda()

    pred_label = torch.argmax(pred, dim=1)
    save_images_cartopy(prediction_video=pred_label,
                        lines=False, high_res_map=False, name='predicted', path=save_images_path)


def run_evaluation(model, hparams, img_X, save_images_path):
    trainer = Trainer(gpus=hparams.num_gpus, distributed_backend=hparams.data_backend,
                      resume_from_checkpoint=hparams.pretrained_path)
    model.freeze()

    #preds = trainer.test(model)

    save_predictions(trainer, model, img_X, save_images_path)


def runModel(img_X):
    sys.argv = ['']

    parser = argparse.ArgumentParser()
    parser.add_argument("--train", default=False, type=str2bool,
                        help="Train model from scratch or use pretrained model for inference")
    parser.add_argument("--lr", default=2e-4, type=float,
                        help="The learning rate")
    parser.add_argument("--beta1", default=0.9, type=float,
                        help="The learning rate")
    parser.add_argument("--beta2", default=0.98, type=float,
                        help="The learning rate")
    parser.add_argument("--eps", default=1e-9, type=float,
                        help="The learning rate")
    parser.add_argument("--loss", default='ce', type=str,
                        help="Loss function to use. Can be either ce, weighted_ce, l1, or combined")
    parser.add_argument("--batch_size", default=12, type=int,
                        help="The batch size. Keep low if limited GPU memory")
    parser.add_argument("--num_gpus", default=0, type=int,
                        help="How many GPU's to use")
    parser.add_argument("--data_backend", default='dp', type=str,
                        help="The data backend. Can be either dp or ddp")
    parser.add_argument("--num_workers", default=0, type=int,
                        help="How many data workers to spawn")
    parser.add_argument("--max_epochs", default=170, type=int,
                        help="How many maximum epochs to run training")
    parser.add_argument("--num_classes", default=4, type=int,
                        help="Number of target classes. Default is 4")
    parser.add_argument("--num_channels_in", default=4, type=int,
                        help="Number of input channels. Can be higher than num_classes if you use additional input channels")
    parser.add_argument("--future_steps", default=4, type=int,
                        help="How many future time steps to predict")
    parser.add_argument("--pretrained_path",
                        default='./models/ae-convlstm.ckpt', type=str)

    hparams = parser.parse_args()

    print('\n')
    print('_________ Initializing ConvLSTM model _________')
    print('--------------------------------------------------------------------------------')
    print('--------------------------------------------------------------------------------')
    print('\n')

    model = ConvLSTMModel(hparams)

    if bool(hparams.train):
        print('Trainining model...')
        train_model(model, hparams)
    else:
        save_images_path = './static/predictions'
        preds = run_evaluation(model, hparams, img_X, save_images_path)
